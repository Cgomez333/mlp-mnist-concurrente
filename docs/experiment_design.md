# Dise√±o Experimental - MLP MNIST Concurrente

Este documento describe el dise√±o experimental completo para el proyecto de implementaci√≥n y paralelizaci√≥n de una Red Neuronal MLP desde cero para clasificaci√≥n de d√≠gitos MNIST.

## üéØ Objetivo Principal

**NO** construir la red neuronal m√°s precisa, sino **analizar y comparar el rendimiento** de diferentes implementaciones secuenciales y paralelas, entendiendo:

- Cuellos de botella computacionales (multiplicaci√≥n de matrices)
- T√©cnicas de paralelismo en CPU (memoria compartida y distribuida)
- Paralelismo masivo en GPU
- M√©tricas: Speedup, Overhead, Ley de Amdahl
- Problem√°tica de transferencia Host-Device en GPGPU

---

## üìä Dataset

### MNIST

- **Entrenamiento**: 60,000 im√°genes
- **Prueba**: 10,000 im√°genes
- **Formato**: 28√ó28 p√≠xeles en escala de grises
- **Clases**: 10 d√≠gitos (0-9)

### Preprocesamiento (Obligatorio)

```
1. Normalizaci√≥n: [0, 255] ‚Üí [0, 1]
2. Aplanamiento: 28√ó28 ‚Üí vector de 784 caracter√≠sticas
3. One-hot encoding: etiquetas ‚Üí vectores de tama√±o 10
4. Mini-batches: dividir dataset en lotes
```

### Formato de Archivos

- **Python**: Usar NumPy arrays (`.npy` o carga directa)
- **C/C++**: Archivos binarios `.bin` (float32, contiguo en memoria)
  - Generados por Python para compatibilidad
  - Formato: `[n_samples, 784]` para X, `[n_samples, 10]` para Y

---

## üß† Arquitectura del MLP (FIJA)

### Estructura de Capas

```
Capa de Entrada:  784 neuronas (fijo)
      ‚Üì
Capa Oculta:      512 neuronas (acordado)
      ‚Üì
Capa de Salida:   10 neuronas (fijo)
```

### Funciones de Activaci√≥n

- **Capa Oculta**: ReLU (Rectified Linear Unit)
  - `ReLU(x) = max(0, x)`
  - `ReLU'(x) = 1 if x > 0 else 0`
- **Capa de Salida**: Softmax
  - `Softmax(z_i) = exp(z_i) / Œ£ exp(z_j)`

### Funci√≥n de P√©rdida

- **Cross-Entropy Loss**
  - `L = -Œ£ y_i * log(≈∑_i)`

### Inicializaci√≥n de Pesos

- **M√©todo**: Xavier/Glorot Uniform
- **SEED fija**: `42` (para reproducibilidad)
- **Precisi√≥n**: `float32` (todas las implementaciones)

---

## ‚öôÔ∏è Hiperpar√°metros (ACORDADOS)

### Par√°metros Globales

```python
EPOCHS = 10
LEARNING_RATE = 0.01
HIDDEN_NEURONS = 512
RANDOM_SEED = 42
```

### Batch Size por Implementaci√≥n

| Implementaci√≥n         | Batch Size | Raz√≥n                         |
| ---------------------- | ---------- | ----------------------------- |
| Python Secuencial      | 64         | Balance memoria/velocidad CPU |
| Python Multiprocessing | 64         | Mismo que secuencial          |
| C Secuencial           | 64         | Consistencia con Python       |
| C OpenMP               | 64         | Mismo que secuencial          |
| PyCUDA (peque√±o)       | 16         | Evaluar latencia GPU          |
| PyCUDA (grande)        | 512        | Evaluar throughput GPU        |

---

## üî¨ Fases de Implementaci√≥n

### Fase 0: Preparaci√≥n (Juntos)

**Responsables**: Ambos  
**Objetivo**: Acordar especificaciones y configurar entorno

**Tareas**:

- [x] Crear estructura del repositorio
- [ ] Definir y documentar arquitectura MLP
- [ ] Fijar hiperpar√°metros
- [ ] Crear scripts de descarga de MNIST
- [ ] Generar archivos binarios para C
- [ ] Definir formato de resultados CSV
- [ ] Configurar `.gitignore` (evitar subir datasets)

**Entregable**: `docs/experiment_design.md` completo

---

### Fase 1: Baseline Secuencial

#### 1A. Python Secuencial (Compa√±ero)

**Carpeta**: `python_secuencial/`

**M√≥dulos a implementar**:

1. **`data.py`**

   - Cargar MNIST
   - Normalizar a [0,1]
   - Aplanar im√°genes (784)
   - One-hot encoding (10 clases)
   - Generador de mini-batches

2. **`model.py`**

   - Inicializaci√≥n de pesos: W1(784√ó512), b1(512), W2(512√ó10), b2(10)
   - `forward(X_batch)`: calcula z1, a1, z2, a2
   - `backward(X_batch, Y_batch)`: calcula gradientes
   - `update_params(lr)`: actualiza pesos

3. **`loss.py`**

   - Cross-Entropy
   - Accuracy

4. **`train.py`**
   - Bucle de entrenamiento (10 epochs)
   - Medici√≥n con `time.perf_counter()`
   - Guardar CSV: `results/raw/python_sequential.csv`

**Validaci√≥n**:

- Loss disminuye de ~2.3 a <0.5
- Accuracy final > 90%

---

#### 1B. C Secuencial (T√∫)

**Carpeta**: `c_secuencial/`

**M√≥dulos a implementar**:

1. **`matrix.c / matrix.h`**

   - Multiplicaci√≥n de matrices (GEMM)
   - ReLU y ReLU' (derivada)
   - Softmax

2. **`mlp.c / mlp.h`**

   - Estructuras para W1, b1, W2, b2
   - `forward()`
   - `backward()`
   - `update_params()`

3. **`data.c / data.h`**

   - Lectura de archivos `.bin` (generados por Python)
   - Estructuras para dataset

4. **`train.c`**
   - Bucle de entrenamiento
   - Medici√≥n con `clock_gettime(CLOCK_MONOTONIC, ...)`
   - Guardar CSV: `results/raw/c_sequential.csv`

**Compilaci√≥n**:

```bash
gcc -O3 -o train_seq *.c -lm
```

**Validaci√≥n**:

- Loss converge similar a Python (diferencia < 1e-4)
- Sin NaN ni overflow

---

### Fase 2: Paralelismo en CPU

#### 2A. Python Multiprocessing (Compa√±ero)

**Carpeta**: `python_multiprocessing/`

**Dise√±o Master-Worker**:

- **Master**: mantiene pesos, divide batches, promedia gradientes
- **Workers**: calculan gradientes en sub-lotes

**Implementaci√≥n**:

- Usar `multiprocessing.Pool` o `Process + Queue`
- Funci√≥n `compute_gradients(params, X_sub, Y_sub)`

**Experimentos**:

```
Procesos: 1, 2, 4, 8
Medir: tiempo total (10 epochs)
CSV: results/raw/python_multiprocessing.csv
Columnas: processes, total_time, speedup_vs_seq
```

---

#### 2B. C + OpenMP (T√∫)

**Carpeta**: `c_openmp/`

**Paralelizaci√≥n**:

```c
// En matrix.c - GEMM
#pragma omp parallel for schedule(static)
for (int i = 0; i < rows; i++) {
    for (int j = 0; j < cols; j++) {
        float sum = 0.0f;
        for (int k = 0; k < inner; k++) {
            sum += A[i*inner + k] * B[k*cols + j];
        }
        C[i*cols + j] = sum;
    }
}
```

**Compilaci√≥n**:

```bash
gcc -O3 -fopenmp -o train_omp *.c -lm
export OMP_NUM_THREADS=4
```

**Experimentos**:

```
Threads: 1, 2, 4, 8
Medir: tiempo total (10 epochs)
CSV: results/raw/c_openmp.csv
Columnas: threads, total_time, speedup_vs_c_seq
```

**Validaci√≥n**:

- Speedup > 1 pero < # threads (por Ley de Amdahl)
- Resultados reproducibles (no race conditions)

---

### Fase 3: GPU con PyCUDA (Compa√±ero)

**Carpeta**: `pycuda_gpu/`  
**Entorno**: Google Colab (GPU T4/P100)

**M√≥dulos**:

1. **`gpu_gemm.py`**

   - Kernel CUDA para multiplicaci√≥n de matrices
   - Compilar con `pycuda.compiler.SourceModule`
   - Funci√≥n `gpu_gemm(A, B)`:
     - Copiar Host‚ÜíDevice
     - Lanzar kernel
     - Copiar Device‚ÜíHost

2. **`gpu_mlp.py`**

   - MLP usando `gpu_gemm` en forward/backward

3. **`train_gpu.py`**
   - Bucle de entrenamiento
   - Medici√≥n con eventos CUDA:
     ```python
     start = cuda.Event()
     end = cuda.Event()
     start.record()
     # ... operaci√≥n ...
     end.record()
     end.synchronize()
     time_ms = start.time_till(end)
     ```

**Experimentos**:

```
Batch sizes: 16, 512
Medir:
  - Tiempo total
  - Tiempo Host‚ÜíDevice
  - Tiempo kernel
  - Tiempo Device‚ÜíHost
CSV: results/raw/pycuda_results.csv
```

**Validaci√≥n**:

- Speedup GPU vs CPU > 5√ó
- Batch 512 m√°s eficiente que 16

---

## üìà M√©tricas y Formato de Resultados

### Estructura de CSV (Todos)

```csv
implementation,language,parallelization,workers_threads,batch_size,epochs,learning_rate,hidden_neurons,total_time_sec,avg_epoch_time,final_loss,final_accuracy,speedup_vs_baseline,notes
python_seq,python,none,1,64,10,0.01,512,45.2,4.52,0.234,0.921,1.00,baseline
python_mp,python,multiprocessing,4,64,10,0.01,512,15.3,1.53,0.235,0.920,2.95,
c_seq,c,none,1,64,10,0.01,512,8.7,0.87,0.233,0.921,1.00,c_baseline
c_openmp,c,openmp,8,64,10,0.01,512,1.9,0.19,0.234,0.920,4.58,
pycuda,python,gpu,1,512,10,0.01,512,2.1,0.21,0.235,0.919,21.52,batch_512
```

### Medici√≥n de Tiempos

**Python**:

```python
import time
start = time.perf_counter()
# ... entrenamiento ...
end = time.perf_counter()
total_time = end - start
```

**C**:

```c
#include <time.h>
struct timespec start, end;
clock_gettime(CLOCK_MONOTONIC, &start);
// ... entrenamiento ...
clock_gettime(CLOCK_MONOTONIC, &end);
double time_spent = (end.tv_sec - start.tv_sec) +
                    (end.tv_nsec - start.tv_nsec) / 1e9;
```

**PyCUDA**:

```python
import pycuda.driver as cuda
start_event = cuda.Event()
end_event = cuda.Event()
start_event.record()
# ... operaci√≥n ...
end_event.record()
end_event.synchronize()
time_ms = start_event.time_till(end_event)
```

---

## üìä An√°lisis y Visualizaci√≥n

### Scripts (Carpeta `scripts/`)

1. **`download_mnist.py`**

   - Descargar dataset autom√°ticamente
   - Guardar en `data/mnist/`

2. **`preprocess_data.py`**

   - Generar archivos `.bin` para C
   - Normalizar y formatear datos

3. **`validate_implementation.py`**

   - Comparar outputs Python vs C (con mismo seed)
   - Verificar diferencias < 1e-4

4. **`aggregate_results.py`**

   - Leer todos los CSV de `results/raw/`
   - Generar tabla consolidada

5. **`plot_results.py`**

   - Gr√°fica: Speedup C+OpenMP vs #threads
   - Gr√°fica: Speedup Python multiprocessing
   - Gr√°fica: Tiempos PyCUDA desglosados (H‚ÜíD, kernel, D‚ÜíH)
   - Gr√°fica: Batch size 16 vs 512 en GPU
   - Gr√°fica: Comparaci√≥n global de todas las implementaciones
   - Guardar en `results/figures/`

6. **`run_all_experiments.sh`**
   - Automatizar todas las ejecuciones

### Gr√°ficas Requeridas

1. **Speedup vs Threads (OpenMP)**
2. **Speedup vs Procesos (Multiprocessing)**
3. **Comparaci√≥n de Tiempos Absolutos (todas las implementaciones)**
4. **Desglose GPU (H‚ÜíD, Kernel, D‚ÜíH)**
5. **Batch Size Impact en GPU**
6. **An√°lisis de Ley de Amdahl**

---

## ‚úÖ Validaci√≥n y Reproducibilidad

### Criterios de √âxito por Fase

**Fase 1 - Baseline**:

- [ ] Loss converge de ~2.3 a <0.5 en 10 epochs
- [ ] Accuracy final > 90%
- [ ] Python y C producen resultados similares (diff < 1e-4)
- [ ] Tiempos registrados correctamente en CSV

**Fase 2 - CPU Paralelo**:

- [ ] Speedup > 1 para todas las configuraciones
- [ ] Speedup < # workers/threads (overhead + Amdahl)
- [ ] Resultados reproducibles (sin race conditions)
- [ ] Ley de Amdahl observable en gr√°ficas

**Fase 3 - GPU**:

- [ ] Speedup GPU vs CPU baseline > 5√ó
- [ ] Batch 512 m√°s r√°pido que batch 16
- [ ] Tiempo kernel > tiempo transferencia
- [ ] Sin errores de memoria GPU

### Protocolo de Validaci√≥n Cruzada

1. Ejecutar Python secuencial con seed=42
2. Guardar pesos finales en `results/raw/weights/python_seq_weights.npy`
3. Ejecutar C secuencial con mismo seed
4. Comparar pesos finales:
   ```python
   diff = np.abs(weights_python - weights_c)
   assert np.max(diff) < 1e-4
   ```

---

## üö´ Restricciones Fundamentales

### NO Permitido

- ‚ùå TensorFlow, Keras, PyTorch, Caffe
- ‚ùå Librer√≠as de Deep Learning pre-construidas
- ‚ùå BLAS/LAPACK en C (implementar GEMM manual)

### Permitido

- ‚úÖ **Python**: NumPy, multiprocessing, PyCUDA, matplotlib
- ‚úÖ **C/C++**: OpenMP, CUDA, librer√≠as est√°ndar (stdio, stdlib, math)

---

## üìÖ Cronograma Sugerido

### Semana 1

- Fase 0 completa (setup + documentaci√≥n)
- Python secuencial implementado y validado
- C secuencial iniciado

### Semana 2

- C secuencial completo
- Validaci√≥n cruzada Python vs C
- Inicio de paralelismo (OpenMP + multiprocessing)

### Semana 3

- OpenMP completo (experimentos con m√∫ltiples threads)
- Multiprocessing completo (experimentos con m√∫ltiples procesos)
- Scripts de an√°lisis (`aggregate_results.py`, `plot_results.py`)

### Semana 4

- PyCUDA implementado y ejecutado en Colab
- Experimentos con batch 16 y 512
- Generaci√≥n de todas las gr√°ficas

### Semana 5 (Buffer)

- Debugging y ajustes finales
- Redacci√≥n del informe t√©cnico
- Preparaci√≥n de presentaci√≥n oral

---

## üì¶ Estructura de Archivos Final

```
mlp-mnist-concurrente/
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îî‚îÄ‚îÄ experiment_design.md          # Este documento
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ mnist/
‚îÇ       ‚îú‚îÄ‚îÄ train-images.bin
‚îÇ       ‚îú‚îÄ‚îÄ train-labels.bin
‚îÇ       ‚îú‚îÄ‚îÄ test-images.bin
‚îÇ       ‚îî‚îÄ‚îÄ test-labels.bin
‚îú‚îÄ‚îÄ python_secuencial/
‚îÇ   ‚îú‚îÄ‚îÄ data.py
‚îÇ   ‚îú‚îÄ‚îÄ model.py
‚îÇ   ‚îú‚îÄ‚îÄ loss.py
‚îÇ   ‚îî‚îÄ‚îÄ train.py
‚îú‚îÄ‚îÄ python_multiprocessing/
‚îÇ   ‚îú‚îÄ‚îÄ data.py
‚îÇ   ‚îú‚îÄ‚îÄ model_parallel.py
‚îÇ   ‚îî‚îÄ‚îÄ train_parallel.py
‚îú‚îÄ‚îÄ pycuda_gpu/
‚îÇ   ‚îú‚îÄ‚îÄ gpu_gemm.py
‚îÇ   ‚îú‚îÄ‚îÄ gpu_mlp.py
‚îÇ   ‚îî‚îÄ‚îÄ train_gpu.py
‚îú‚îÄ‚îÄ c_secuencial/
‚îÇ   ‚îú‚îÄ‚îÄ include/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ matrix.h
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ mlp.h
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ data.h
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ matrix.c
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ mlp.c
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data.c
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ train.c
‚îÇ   ‚îî‚îÄ‚îÄ Makefile
‚îú‚îÄ‚îÄ c_openmp/
‚îÇ   ‚îú‚îÄ‚îÄ include/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ matrix.h
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ mlp.h
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ data.h
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ matrix_omp.c
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ mlp_omp.c
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data.c
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ train_omp.c
‚îÇ   ‚îî‚îÄ‚îÄ Makefile
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ download_mnist.py
‚îÇ   ‚îú‚îÄ‚îÄ preprocess_data.py
‚îÇ   ‚îú‚îÄ‚îÄ validate_implementation.py
‚îÇ   ‚îú‚îÄ‚îÄ aggregate_results.py
‚îÇ   ‚îú‚îÄ‚îÄ plot_results.py
‚îÇ   ‚îî‚îÄ‚îÄ run_all_experiments.sh
‚îî‚îÄ‚îÄ results/
    ‚îú‚îÄ‚îÄ raw/
    ‚îÇ   ‚îú‚îÄ‚îÄ python_sequential.csv
    ‚îÇ   ‚îú‚îÄ‚îÄ python_multiprocessing.csv
    ‚îÇ   ‚îú‚îÄ‚îÄ c_sequential.csv
    ‚îÇ   ‚îú‚îÄ‚îÄ c_openmp.csv
    ‚îÇ   ‚îú‚îÄ‚îÄ pycuda_results.csv
    ‚îÇ   ‚îî‚îÄ‚îÄ weights/
    ‚îÇ       ‚îú‚îÄ‚îÄ python_seq_final.npy
    ‚îÇ       ‚îî‚îÄ‚îÄ c_seq_final.bin
    ‚îî‚îÄ‚îÄ figures/
        ‚îú‚îÄ‚îÄ speedup_openmp.png
        ‚îú‚îÄ‚îÄ speedup_multiprocessing.png
        ‚îú‚îÄ‚îÄ comparison_all.png
        ‚îú‚îÄ‚îÄ gpu_breakdown.png
        ‚îú‚îÄ‚îÄ batch_size_comparison.png
        ‚îî‚îÄ‚îÄ amdahl_analysis.png
```

---

## üë• Divisi√≥n de Responsabilidades

### Compa√±ero (Python + PyCUDA)

- ‚úÖ Python secuencial completo
- ‚úÖ Python multiprocessing (master-worker)
- ‚úÖ PyCUDA (kernels CUDA + experimentos en Colab)
- ‚úÖ Scripts de descarga y preprocesamiento de datos
- ‚úÖ Generaci√≥n de archivos `.bin` para C
- ‚úÖ Secciones del informe: metodolog√≠a Python, resultados GPU
- ‚úÖ Presentaci√≥n: MLP b√°sico, multiprocessing, PyCUDA

### T√∫ (C + OpenMP)

- ‚úÖ C secuencial completo (GEMM manual, MLP from scratch)
- ‚úÖ C OpenMP (paralelizaci√≥n de bucles cr√≠ticos)
- ‚úÖ Validaci√≥n de convergencia num√©rica
- ‚úÖ Experimentos con m√∫ltiples threads
- ‚úÖ Secciones del informe: C implementation, OpenMP, an√°lisis Amdahl
- ‚úÖ Presentaci√≥n: C secuencial, OpenMP speedup, comparaci√≥n CPU vs GPU

### Ambos (Colaborativo)

- ‚úÖ Fase 0: definici√≥n de arquitectura e hiperpar√°metros
- ‚úÖ Scripts de an√°lisis y visualizaci√≥n
- ‚úÖ Validaci√≥n cruzada de implementaciones
- ‚úÖ Generaci√≥n de gr√°ficas finales
- ‚úÖ Revisi√≥n del informe completo
- ‚úÖ Ensayo de presentaci√≥n

---

## üìö Fundamentaci√≥n Matem√°tica (Referencia)

### Forward Propagation

```
Z1 = X @ W1 + b1          # (batch, 784) @ (784, 512) = (batch, 512)
A1 = ReLU(Z1)             # (batch, 512)
Z2 = A1 @ W2 + b2         # (batch, 512) @ (512, 10) = (batch, 10)
A2 = Softmax(Z2)          # (batch, 10) - probabilidades
```

### Loss

```
L = -Œ£ (Y * log(A2)) / batch_size
```

### Backward Propagation

```
dZ2 = A2 - Y              # (batch, 10)
dW2 = A1^T @ dZ2          # (512, batch) @ (batch, 10) = (512, 10)
db2 = sum(dZ2, axis=0)    # (10,)

dA1 = dZ2 @ W2^T          # (batch, 10) @ (10, 512) = (batch, 512)
dZ1 = dA1 * ReLU'(Z1)     # (batch, 512) element-wise
dW1 = X^T @ dZ1           # (784, batch) @ (batch, 512) = (784, 512)
db1 = sum(dZ1, axis=0)    # (512,)
```

### Update

```
W1 = W1 - lr * dW1
b1 = b1 - lr * db1
W2 = W2 - lr * dW2
b2 = b2 - lr * db2
```

---

## üìû Contacto y Soporte

- **Reuniones de sincronizaci√≥n**: Semanal (presencial o call)
- **Validaci√≥n de c√≥digo**: Antes de cada merge a `main`
- **Dudas t√©cnicas**: Compartir en grupo o consultar con profesor

---

**√öltima actualizaci√≥n**: 26 de noviembre de 2025  
**Versi√≥n**: 1.0  
**Estado**: ‚úÖ Aprobado por ambos miembros del equipo
