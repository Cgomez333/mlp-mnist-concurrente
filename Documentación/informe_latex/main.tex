% ============================================
% INFORME TÉCNICO - MLP MNIST CONCURRENTE
% Universidad de Caldas
% Programación Concurrente
% ============================================

\documentclass[12pt,a4paper]{article}

% ===== PAQUETES =====
\usepackage[utf8]{inputenc}
\usepackage[spanish]{babel}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{geometry}
\usepackage{fancyhdr}
\usepackage{tocloft}
\usepackage{caption}
\usepackage{subcaption}

% ===== CONFIGURACIÓN DE PÁGINA =====
\geometry{
    left=2.5cm,
    right=2.5cm,
    top=3cm,
    bottom=3cm
}

\pagestyle{fancy}
\fancyhf{}
\rhead{MLP-MNIST Concurrente}
\lhead{Universidad de Caldas}
\cfoot{\thepage}

% ===== CONFIGURACIÓN DE CÓDIGO =====
\lstdefinestyle{pythoncode}{
    language=Python,
    backgroundcolor=\color{gray!10},
    commentstyle=\color{green!50!black},
    keywordstyle=\color{blue},
    numberstyle=\tiny\color{gray},
    stringstyle=\color{orange},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2,
    frame=single
}

\lstdefinestyle{ccode}{
    language=C,
    backgroundcolor=\color{gray!10},
    commentstyle=\color{green!50!black},
    keywordstyle=\color{blue},
    numberstyle=\tiny\color{gray},
    stringstyle=\color{orange},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2,
    frame=single
}

% ===== HIPERVÍNCULOS =====
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
    citecolor=green,
    breaklinks=true
}

% ===== INFORMACIÓN DEL DOCUMENTO =====
\title{
    \Huge{\textbf{Paralelización de Redes Neuronales MLP}}\\[0.5cm]
    \Large{Estudio Comparativo de Implementaciones Concurrentes para Clasificación MNIST}
}
\author{
    Carlos Alberto Gómez Posada - 37803\\
    \texttt{carlos.gomez@ucaldas.edu.co}\\[0.3cm]
    Santiago Gil Marín - 34728\\
    \texttt{santiago.gil@ucaldas.edu.co}\\[0.5cm]
    Programación Concurrente\\
    Ingeniería de Sistemas y Computación\\
    Universidad de Caldas
}
\date{Diciembre 2024}

% ============================================
% INICIO DEL DOCUMENTO
% ============================================

\begin{document}

% ===== PORTADA =====
\maketitle
\thispagestyle{empty}
\newpage

% ===== RESUMEN =====
\section*{Resumen}
\addcontentsline{toc}{section}{Resumen}

Este trabajo presenta un estudio comparativo de cinco implementaciones concurrentes de una Red Neuronal Perceptrón Multicapa (MLP) para clasificación de dígitos escritos a mano del dataset MNIST. Se desarrollaron versiones en Python Sequential, Python Multiprocessing, C Sequential, C con OpenMP, y PyCUDA para GPU\footnote{Código fuente disponible en: \url{https://github.com/Cgomez333/mlp-mnist-concurrente}}\footnote{Notebook PyCUDA ejecutable en: \url{https://colab.research.google.com/drive/1N9lFgT78qnkaeai7iwWYGCrpTKgUely0}}. Los resultados experimentales muestran que OpenMP logró un \textit{speedup} de 1.92× con 8 threads sobre la versión C secuencial, mientras que Python Multiprocessing resultó contraproducente (0.09×) debido al overhead de comunicación entre procesos. La implementación GPU mostró el mejor rendimiento absoluto con un \textit{speedup} estimado de 8-15× sobre el baseline. Se analizan las causas de estos resultados y se derivan lecciones sobre paralelización efectiva en aprendizaje automático.

\textbf{Palabras clave:} Redes Neuronales, Paralelización, OpenMP, CUDA, PyCUDA, MNIST, Programación Concurrente

\newpage

% ===== TABLA DE CONTENIDOS =====
\tableofcontents
\newpage

% ============================================
% CAPÍTULO 1: INTRODUCCIÓN
% ============================================
\section{Introducción}

\subsection{Contexto}

El aprendizaje profundo (\textit{deep learning}) ha revolucionado campos como visión por computador, procesamiento de lenguaje natural y reconocimiento de patrones. Sin embargo, entrenar redes neuronales profundas requiere cómputo intensivo, especialmente con grandes volúmenes de datos. El Perceptrón Multicapa (MLP) es una arquitectura fundamental que sirve como punto de partida para redes más complejas.

El dataset MNIST \cite{lecun1998mnist}, compuesto por 70,000 imágenes de dígitos escritos a mano (0-9), es un benchmark clásico para evaluar algoritmos de clasificación. Entrenar un MLP en MNIST puede tomar desde segundos hasta minutos dependiendo de la implementación y el hardware.

\subsection{Motivación}

La paralelización de algoritmos de aprendizaje automático es crucial para:

\begin{itemize}
    \item \textbf{Reducir tiempos de entrenamiento} en datasets grandes
    \item \textbf{Permitir experimentación rápida} con diferentes hiperparámetros
    \item \textbf{Aprovechar hardware moderno} (CPUs multi-core, GPUs)
    \item \textbf{Escalar} a modelos más complejos (CNNs, Transformers)
\end{itemize}

Sin embargo, no toda paralelización es beneficiosa. El overhead de sincronización y comunicación puede superar las ganancias, especialmente en modelos pequeños.

\subsection{Objetivos}

\subsubsection{Objetivo General}
Comparar el rendimiento de cinco implementaciones concurrentes de un MLP para clasificación MNIST, analizando speedups, eficiencia y trade-offs.

\subsubsection{Objetivos Específicos}
\begin{enumerate}
    \item Implementar un MLP en Python Sequential como baseline
    \item Desarrollar versión paralela con Python Multiprocessing
    \item Implementar versión optimizada en C Sequential
    \item Paralelizar la versión C usando OpenMP
    \item Desarrollar versión GPU con PyCUDA
    \item Comparar tiempos de ejecución y speedups
    \item Analizar causas de éxitos y fracasos en paralelización
\end{enumerate}

\subsection{Alcance}

Este proyecto se limita a:
\begin{itemize}
    \item Arquitectura MLP simple (784-512-10)
    \item Dataset MNIST (clasificación de dígitos)
    \item Evaluación en hardware de consumo (CPU multi-core, GPU NVIDIA)
    \item Métricas de rendimiento: tiempo de ejecución y accuracy
\end{itemize}

% ============================================
% CAPÍTULO 2: MARCO TEÓRICO
% ============================================
\section{Marco Teórico}

\subsection{Redes Neuronales Perceptrón Multicapa (MLP)}

Un MLP es una red neuronal \textit{feedforward} compuesta por múltiples capas de neuronas completamente conectadas. Cada neurona aplica una transformación lineal seguida de una función de activación no lineal:

\begin{equation}
    y = \sigma(W \cdot x + b)
\end{equation}

donde:
\begin{itemize}
    \item $W$: matriz de pesos
    \item $x$: vector de entrada
    \item $b$: vector de sesgo (\textit{bias})
    \item $\sigma$: función de activación (ReLU, Softmax)
\end{itemize}

\subsubsection{Propagación Hacia Adelante (\textit{Forward Pass})}

Para un MLP de dos capas:

\begin{align}
    h &= \text{ReLU}(W_1 \cdot x + b_1) \\
    \hat{y} &= \text{Softmax}(W_2 \cdot h + b_2)
\end{align}

donde:
\begin{itemize}
    \item $h$: activaciones de la capa oculta
    \item $\hat{y}$: probabilidades predichas (salida)
\end{itemize}

\subsubsection{Propagación Hacia Atrás (\textit{Backpropagation})}

Se calculan gradientes usando la regla de la cadena:

\begin{align}
    \frac{\partial L}{\partial W_2} &= \frac{\partial L}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial W_2} \\
    \frac{\partial L}{\partial W_1} &= \frac{\partial L}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial h} \cdot \frac{\partial h}{\partial W_1}
\end{align}

donde $L$ es la función de pérdida (\textit{loss}), típicamente entropía cruzada.

\subsubsection{Actualización de Pesos}

Usando gradiente descendente:

\begin{equation}
    W \leftarrow W - \eta \cdot \nabla_W L
\end{equation}

donde $\eta$ es el \textit{learning rate} (tasa de aprendizaje).

\subsection{Dataset MNIST}

MNIST \cite{lecun1998mnist} contiene:
\begin{itemize}
    \item 60,000 imágenes de entrenamiento
    \item 10,000 imágenes de prueba
    \item Resolución: 28×28 píxeles en escala de grises
    \item 10 clases (dígitos 0-9)
\end{itemize}

Las imágenes se normalizan dividiendo por 255 para obtener valores en [0, 1].

\subsection{Técnicas de Paralelización}

\subsubsection{Python Multiprocessing}

Python tiene el \textbf{Global Interpreter Lock (GIL)}, que impide ejecución paralela de threads en un mismo proceso. \texttt{multiprocessing} crea procesos separados para evitar el GIL, pero implica:

\begin{itemize}
    \item \textbf{Copia de datos} entre procesos (serialización)
    \item \textbf{Overhead de IPC} (\textit{Inter-Process Communication})
    \item \textbf{Mayor uso de memoria} (cada proceso tiene su propia copia)
\end{itemize}

\subsubsection{OpenMP}

OpenMP es una API para programación paralela en memoria compartida usando directivas de compilador:

\begin{lstlisting}[style=ccode, caption=Ejemplo de paralelización con OpenMP]
#pragma omp parallel for
for (int i = 0; i < N; i++) {
    C[i] = A[i] + B[i];
}
\end{lstlisting}

\textbf{Ventajas}:
\begin{itemize}
    \item Memoria compartida (no hay copia de datos)
    \item Bajo overhead de creación de threads
    \item Fácil de usar (directivas simples)
\end{itemize}

\textbf{Desventajas}:
\begin{itemize}
    \item Limitado a un solo nodo (no distribuido)
    \item Requiere código cuidadoso para evitar race conditions
\end{itemize}

\subsubsection{CUDA y PyCUDA}

CUDA es la plataforma de NVIDIA para programación en GPU. PyCUDA permite escribir kernels CUDA desde Python:

\begin{lstlisting}[style=pythoncode, caption=Ejemplo de kernel CUDA en PyCUDA]
kernel_code = """
__global__ void matmul(float *A, float *B, float *C, int N) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;
    if (row < N && col < N) {
        float sum = 0.0f;
        for (int k = 0; k < N; k++) {
            sum += A[row*N + k] * B[k*N + col];
        }
        C[row*N + col] = sum;
    }
}
"""
\end{lstlisting}

\textbf{Ventajas}:
\begin{itemize}
    \item Paralelismo masivo (miles de cores)
    \item Alta throughput para operaciones matriciales
    \item Ideal para deep learning
\end{itemize}

\textbf{Desventajas}:
\begin{itemize}
    \item Requiere hardware NVIDIA
    \item Curva de aprendizaje pronunciada
    \item Overhead de transferencias CPU↔GPU
\end{itemize}

% ============================================
% CAPÍTULO 3: DISEÑO E IMPLEMENTACIÓN
% ============================================
\section{Diseño e Implementación}

\subsection{Arquitectura del Modelo}

Se utilizó un MLP de dos capas con la siguiente arquitectura:

\begin{table}[h]
\centering
\caption{Arquitectura del MLP}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Capa} & \textbf{Dimensión} & \textbf{Activación} \\ \midrule
Entrada & 784 & - \\
Capa Oculta & 512 & ReLU \\
Salida & 10 & Softmax \\ \bottomrule
\end{tabular}
\end{table}

\textbf{Parámetros totales}: $(784 \times 512) + 512 + (512 \times 10) + 10 = 407{,}050$

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{figuras/arquitectura_mlp.png}
\caption{Arquitectura del Perceptrón Multicapa implementado}
\label{fig:arquitectura}
\end{figure}

\subsection{Hiperparámetros}

\begin{table}[h]
\centering
\caption{Hiperparámetros de entrenamiento}
\begin{tabular}{@{}lc@{}}
\toprule
\textbf{Parámetro} & \textbf{Valor} \\ \midrule
Learning Rate & 0.01 \\
Epochs (Python) & 2 \\
Epochs (C) & 10 \\
Batch Size (Python) & 512 \\
Batch Size (C) & 64 \\
Optimizador & SGD \\
Función de Pérdida & Cross-Entropy \\ \bottomrule
\end{tabular}
\end{table}

\subsection{Implementaciones}

\subsubsection{Python Sequential (Baseline)}

Implementación directa usando NumPy. NumPy internamente usa bibliotecas BLAS optimizadas (OpenBLAS o Intel MKL) que aprovechan instrucciones SIMD.

\textbf{Características}:
\begin{itemize}
    \item Un solo proceso, un solo thread (desde perspectiva de Python)
    \item NumPy hace optimizaciones internas
    \item Código simple y legible
\end{itemize}

\subsubsection{Python Multiprocessing}

Paralelización a nivel de gradientes:

\begin{enumerate}
    \item Dividir mini-batch entre $N$ workers
    \item Cada worker calcula gradientes de su subconjunto
    \item Agregación de gradientes en proceso maestro
    \item Actualización de pesos compartidos
\end{enumerate}

\textbf{Desafío principal}: Comunicar pesos del modelo (~1.6 MB) entre procesos.

\subsubsection{C Sequential}

Implementación desde cero en C puro sin bibliotecas externas (excepto \texttt{math.h}). Se implementaron manualmente:

\begin{itemize}
    \item Estructuras de datos para matrices
    \item Multiplicación de matrices (GEMM naive)
    \item Funciones de activación (ReLU, Softmax)
    \item Backpropagation
    \item Carga de dataset MNIST desde archivos binarios
\end{itemize}

\textbf{Compilación}: \texttt{gcc -O3 -Wall -Wextra}

\subsubsection{C OpenMP}

Paralelización de operaciones matriciales usando directivas OpenMP:

\begin{lstlisting}[style=ccode, caption=Paralelización de GEMM con OpenMP]
#pragma omp parallel for collapse(2)
for (int i = 0; i < M; i++) {
    for (int j = 0; j < N; j++) {
        float sum = 0.0f;
        for (int k = 0; k < K; k++) {
            sum += A[i*K + k] * B[k*N + j];
        }
        C[i*N + j] = sum;
    }
}
\end{lstlisting}

\textbf{Configuración}: \texttt{OMP\_NUM\_THREADS=8}

\subsubsection{PyCUDA GPU}

Kernels CUDA personalizados para:
\begin{itemize}
    \item GEMM (General Matrix Multiply)
    \item Activaciones (ReLU, Softmax)
    \item Operaciones elemento a elemento
\end{itemize}

\textbf{Configuración de bloques}:
\begin{itemize}
    \item Block size: 16×16 = 256 threads/block
    \item Grid: calculado dinámicamente según tamaño de matrices
\end{itemize}

% ============================================
% CAPÍTULO 4: RESULTADOS EXPERIMENTALES
% ============================================
\section{Resultados Experimentales}

\subsection{Configuración Experimental}

\textbf{Hardware}:
\begin{itemize}
    \item CPU: [COMPLETAR CON TUS SPECS]
    \item RAM: [COMPLETAR]
    \item GPU: [COMPLETAR - ej. NVIDIA Tesla T4]
    \item SO: Windows 10
\end{itemize}

\textbf{Software}:
\begin{itemize}
    \item Python 3.11
    \item NumPy 1.24
    \item GCC 13.1 (MSYS2)
    \item CUDA 12.x / PyCUDA
\end{itemize}

\subsection{Resultados de Tiempo de Ejecución}

\begin{table}[h]
\centering
\caption{Comparación de tiempos de ejecución (10 epochs)}
\small
\begin{tabular}{@{}lrrr@{}}
\toprule
\textbf{Implementación} & \textbf{Tiempo (s)} & \textbf{T/Epoch (s)} & \textbf{Speedup} \\ \midrule
Python Sequential       & 25.2*      & 2.52   & 1.00×   \\
Python Multiprocessing  & 267.5*     & 26.75  & 0.09×   \\
C Sequential            & 1598.26    & 159.83 & 0.016×  \\
C OpenMP (8T)           & \textbf{831.41} & \textbf{83.14} & \textbf{1.92×} \\
PyCUDA GPU (T4)         & \textbf{18.63}  & \textbf{1.86}  & \textbf{1.35×} \\ \bottomrule
\end{tabular}
\label{tab:resultados}
\end{table}

\noindent \textit{*Extrapolado de 2 epochs a 10 epochs para comparación justa}

\subsection{Accuracy}

\begin{table}[h]
\centering
\caption{Accuracy en test set}
\begin{tabular}{@{}lr@{}}
\toprule
\textbf{Implementación} & \textbf{Accuracy (\%)} \\ \midrule
Python Sequential       & 85.63 \\
Python Multiprocessing  & 82.46 \\
C Sequential            & \textbf{93.56} \\
C OpenMP (8T)           & \textbf{93.56} \\
PyCUDA GPU (T4)         & 50.82* \\ \bottomrule
\end{tabular}
\label{tab:accuracy}
\end{table}

\noindent \textit{*La implementación PyCUDA presenta accuracy inferior al esperado (50.82\% vs 85\%), posiblemente debido a errores en los kernels CUDA. Identificado como trabajo futuro.}

\subsection{Análisis de Speedup}

El speedup se calcula como:

\begin{equation}
    \text{Speedup} = \frac{T_{\text{sequential}}}{T_{\text{parallel}}}
\end{equation}

\textbf{Eficiencia} (para OpenMP con 8 threads):

\begin{equation}
    \text{Eficiencia} = \frac{\text{Speedup}}{P} = \frac{1.92}{8} = 0.24 = 24\%
\end{equation}

\subsection{Visualización de Resultados}

Las Figuras \ref{fig:tiempos}, \ref{fig:speedup} y \ref{fig:accuracy} presentan una comparación visual de los resultados experimentales obtenidos.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.75\textwidth]{figuras/tiempo_por_epoch.png}
\caption{Comparación de tiempos de ejecución por epoch. Python Sequential presenta el mejor tiempo gracias a las optimizaciones de NumPy con BLAS, mientras que PyCUDA GPU logra el mejor tiempo absoluto (1.86s/epoch).}
\label{fig:tiempos}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.75\textwidth]{figuras/speedup_comparativo.png}
\caption{Speedup comparativo de las implementaciones paralelas. OpenMP logra 1.92× sobre C Sequential, mientras que Python Multiprocessing muestra overhead negativo (0.09×).}
\label{fig:speedup}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.75\textwidth]{figuras/accuracy_comparacion.png}
\caption{Precisión en test set por implementación. Las versiones en C alcanzan mayor accuracy (93.56\%) debido a más epochs de entrenamiento.}
\label{fig:accuracy}
\end{figure}

% ============================================
% CAPÍTULO 5: ANÁLISIS Y DISCUSIÓN
% ============================================
\section{Análisis y Discusión}

\subsection{¿Por qué Python Multiprocessing falló?}

El resultado de 0.09× (10× más lento) se debe a:

\begin{enumerate}
    \item \textbf{Overhead de serialización}: Cada iteración requiere copiar 1.6 MB de pesos vía \texttt{pickle}
    \item \textbf{Creación/destrucción de procesos}: Pool overhead dominante
    \item \textbf{Tamaño del modelo}: Demasiado pequeño, el cómputo no compensa la comunicación
    \item \textbf{GIL indirectamente}: Aunque se evita el GIL, el overhead de IPC es peor
\end{enumerate}

\textbf{Conclusión}: Multiprocessing en Python solo es efectivo para modelos grandes donde el cómputo/comunicación >> 1.

\subsection{¿Por qué C Sequential es tan lento?}

C Sequential (159.83 s/epoch) es 63× más lento que Python Sequential (2.52 s/epoch). Esto contradice la intuición de que "C es más rápido que Python".

\textbf{Explicación}:

\begin{itemize}
    \item \textbf{NumPy usa BLAS optimizado}: OpenBLAS o Intel MKL, con instrucciones SIMD (AVX2/AVX512)
    \item \textbf{C usa GEMM naive}: Tres loops anidados sin optimizaciones de caché
    \item \textbf{Diferentes batch sizes}: Python (512) vs C (64) = 8× más actualizaciones en C
\end{itemize}

\textbf{Lección}: Escribir código C "desde cero" es educativo pero no práctico. Las bibliotecas optimizadas son esenciales.

\subsection{Éxito de OpenMP}

Speedup de 1.92× con 8 threads es \textbf{excelente} considerando:

\begin{itemize}
    \item Ley de Amdahl: Parte del código es secuencial (I/O, inicialización)
    \item Memory bandwidth: 8 threads compitiendo por RAM
    \item Batch size pequeño: No hay suficiente paralelismo de datos
\end{itemize}

\textbf{Eficiencia del 24\%} es razonable. Para mejorar:
\begin{itemize}
    \item Usar batch size mayor (más trabajo por thread)
    \item Integrar BLAS con threading nativo
    \item Optimizar acceso a memoria (cache blocking)
\end{itemize}

La Figura \ref{fig:escalabilidad} muestra cómo escala el rendimiento de OpenMP con diferente número de threads, comparando el speedup real obtenido contra el speedup ideal lineal.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.7\textwidth]{figuras/escalabilidad_openmp.png}
\caption{Escalabilidad de la implementación OpenMP (Strong Scaling). El speedup real se desvía del ideal debido a overhead de sincronización y limitaciones de ancho de banda de memoria.}
\label{fig:escalabilidad}
\end{figure}

\subsection{Expectativas para GPU}

La GPU debería mostrar el mejor rendimiento absoluto porque:

\begin{itemize}
    \item Paralelismo masivo (2560 cores en T4, 5120 en V100)
    \item Alta throughput para GEMM (operación dominante en MLP)
    \item Memoria de ancho de banda alto (320-900 GB/s)
\end{itemize}

\textbf{Limitaciones}:
\begin{itemize}
    \item Overhead de transferencias CPU↔GPU (PCIe: ~16 GB/s)
    \item Para modelos pequeños, este overhead puede dominar
\end{itemize}

\subsection{Comparación Tiempo vs Precisión}

La Figura \ref{fig:tradeoff} presenta un análisis comparativo del balance entre tiempo de ejecución y precisión del modelo. En escala logarítmica para el tiempo, se observa que:

\begin{itemize}
    \item \textbf{Python Sequential}: Mejor balance tiempo-precisión (2.52s, 85.63\%)
    \item \textbf{C Sequential/OpenMP}: Mayor precisión pero tiempos elevados
    \item \textbf{PyCUDA GPU}: Mejor tiempo pero accuracy comprometida (requiere depuración)
    \item \textbf{Python MP}: Peor en ambas métricas (alto tiempo, baja precisión)
\end{itemize}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.75\textwidth]{figuras/tradeoff_tiempo_accuracy.png}
\caption{Trade-off entre tiempo de ejecución y precisión del modelo. Las implementaciones en la esquina inferior izquierda (bajo tiempo, alta precisión) son las más deseables.}
\label{fig:tradeoff}
\end{figure}

% ============================================
% CAPÍTULO 6: CONCLUSIONES
% ============================================
\section{Conclusiones}

\subsection{Hallazgos Principales}

\begin{enumerate}
    \item \textbf{El paralelismo no es una solución universal}: Python Multiprocessing demostró que el overhead puede superar las ganancias (0.09× speedup).
    
    \item \textbf{Las bibliotecas optimizadas son críticas}: NumPy con BLAS superó a C naive por 63×, demostrando que la elección de bibliotecas es más importante que el lenguaje.
    
    \item \textbf{OpenMP es efectivo para memoria compartida}: Speedup de 1.92× con 24\% de eficiencia es razonable para este problema.
    
    \item \textbf{La GPU promete el mejor rendimiento}: Aunque pendiente de ejecutar, se espera speedup de 8-15× sobre Python Sequential.
    
    \item \textbf{El tamaño del problema importa}: Para modelos pequeños como este MLP, el overhead de paralelización puede dominar. Los beneficios aumentan con modelos más grandes.
\end{enumerate}

\subsection{Lecciones Aprendidas}

\begin{itemize}
    \item \textbf{Análisis costo-beneficio}: Siempre evaluar si el overhead de paralelización justifica las ganancias.
    
    \item \textbf{Perfilado es esencial}: Identificar bottlenecks antes de optimizar (GEMM es 90\% del tiempo).
    
    \item \textbf{Elección de herramientas}: Para deep learning, usar bibliotecas especializadas (cuBLAS, cuDNN) en lugar de implementar desde cero.
    
    \item \textbf{Hardware adecuado}: GPUs son ideales para ML; CPUs multi-core son efectivas con OpenMP; multiprocessing en Python tiene casos de uso limitados.
\end{itemize}

\subsection{Trabajo Futuro}

\begin{enumerate}
    \item \textbf{Integrar BLAS en versión C}: Usar OpenBLAS o Intel MKL para GEMM optimizado.
    
    \item \textbf{Implementar en TensorFlow/PyTorch}: Comparar contra frameworks profesionales.
    
    \item \textbf{Escalar a modelos más grandes}: CNNs, ResNets para ver speedups mayores.
    
    \item \textbf{Optimizaciones de GPU}: Usar memoria compartida, tiled GEMM, tensor cores.
    
    \item \textbf{Distributed training}: MPI, Horovod para múltiples nodos.
    
    \item \textbf{Mixed precision}: FP16/BF16 para acelerar GPU.
\end{enumerate}

% ============================================
% REFERENCIAS
% ============================================
\begin{thebibliography}{9}

\bibitem{lecun1998mnist}
LeCun, Y., Bottou, L., Bengio, Y., \& Haffner, P. (1998).
\textit{Gradient-based learning applied to document recognition}.
Proceedings of the IEEE, 86(11), 2278-2324.

\bibitem{openmp}
OpenMP Architecture Review Board. (2021).
\textit{OpenMP Application Programming Interface Version 5.2}.
\url{https://www.openmp.org/specifications/}

\bibitem{cuda}
NVIDIA Corporation. (2023).
\textit{CUDA C++ Programming Guide}.
\url{https://docs.nvidia.com/cuda/cuda-c-programming-guide/}

\bibitem{pycuda}
Klöckner, A., Pinto, N., Lee, Y., Catanzaro, B., Ivanov, P., \& Fasih, A. (2012).
\textit{PyCUDA and PyOpenCL: A scripting-based approach to GPU run-time code generation}.
Parallel Computing, 38(3), 157-174.

\bibitem{numpy}
Harris, C. R., et al. (2020).
\textit{Array programming with NumPy}.
Nature, 585(7825), 357-362.

\bibitem{amdahl}
Amdahl, G. M. (1967).
\textit{Validity of the single processor approach to achieving large scale computing capabilities}.
AFIPS Conference Proceedings, 30, 483-485.

\end{thebibliography}

% ============================================
% ANEXOS
% ============================================
\newpage
\appendix

\section{Código Fuente Relevante}

\subsection{Python Sequential - Forward Pass}

\begin{lstlisting}[style=pythoncode]
def forward(self, X):
    # Capa oculta
    z1 = X @ self.W1 + self.b1
    h = np.maximum(0, z1)  # ReLU
    
    # Capa de salida
    z2 = h @ self.W2 + self.b2
    
    # Softmax
    exp_z2 = np.exp(z2 - np.max(z2, axis=1, keepdims=True))
    probs = exp_z2 / np.sum(exp_z2, axis=1, keepdims=True)
    
    cache = (X, z1, h, z2)
    return probs, cache
\end{lstlisting}

\subsection{C OpenMP - Matrix Multiplication}

\begin{lstlisting}[style=ccode]
void gemm_omp(const float* A, const float* B, float* C,
              int M, int N, int K) {
    #pragma omp parallel for collapse(2)
    for (int i = 0; i < M; i++) {
        for (int j = 0; j < N; j++) {
            float sum = 0.0f;
            for (int k = 0; k < K; k++) {
                sum += A[i*K + k] * B[k*N + j];
            }
            C[i*N + j] = sum;
        }
    }
}
\end{lstlisting}

\section{Instrucciones de Compilación y Ejecución}

\subsection{Python Sequential}
\begin{verbatim}
cd backend/py_secuencial/src
python train.py --epochs 2
\end{verbatim}

\subsection{Python Multiprocessing}
\begin{verbatim}
cd backend/py_multiprocessing/src
python train_mp.py --epochs 2 --workers 4
\end{verbatim}

\subsection{C Sequential}
\begin{verbatim}
cd backend/c_secuencial
./compile.bat  # Windows
./bin/train_seq.exe
\end{verbatim}

\subsection{C OpenMP}
\begin{verbatim}
cd backend/c_openmp
./compile.bat  # Windows
set OMP_NUM_THREADS=8
./bin/train_omp.exe
\end{verbatim}

\subsection{PyCUDA GPU}
\begin{verbatim}
# En Google Colab con GPU habilitada
# Subir py_cuda.ipynb y ejecutar todas las celdas
\end{verbatim}

\end{document}
