# MLP MNIST - ImplementaciÃ³n en C Secuencial

Esta carpeta contiene la implementaciÃ³n baseline en C puro (sin paralelizaciÃ³n) del MLP para MNIST.

## ğŸ“ Estructura

```
c_secuencial/
â”œâ”€â”€ include/
â”‚   â”œâ”€â”€ data.h      # Carga de datos desde archivos .bin
â”‚   â”œâ”€â”€ matrix.h    # Operaciones de matrices
â”‚   â””â”€â”€ mlp.h       # Estructura y funciones del MLP
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data.c      # ImplementaciÃ³n de carga de datos
â”‚   â”œâ”€â”€ matrix.c    # ImplementaciÃ³n de operaciones matriciales
â”‚   â”œâ”€â”€ mlp.c       # ImplementaciÃ³n del MLP (forward, backward, update)
â”‚   â””â”€â”€ train.c     # Programa principal de entrenamiento
â”œâ”€â”€ Makefile        # CompilaciÃ³n automatizada
â””â”€â”€ README.md       # Este archivo
```

## ğŸ¯ Especificaciones

- **Arquitectura**: 784 â†’ 512 (ReLU) â†’ 10 (Softmax)
- **Loss**: Cross-Entropy
- **Optimizador**: Gradient Descent
- **HiperparÃ¡metros**:
  - Epochs: 10
  - Batch size: 64
  - Learning rate: 0.01
  - Seed: 42

## ğŸ”§ CompilaciÃ³n

### Requisitos

- GCC (versiÃ³n 7.0 o superior)
- Make
- LibrerÃ­a matemÃ¡tica estÃ¡ndar (libm)

### Compilar

```bash
make
```

Esto genera el ejecutable en `bin/train_seq`.

### Compilar y ejecutar

```bash
make run
```

### Limpiar archivos compilados

```bash
make clean
```

## â–¶ï¸ EjecuciÃ³n

```bash
cd bin
./train_seq
```

O simplemente:

```bash
make run
```

## ğŸ“Š Salida Esperada

```
=================================================================
MLP MNIST - ImplementaciÃ³n Secuencial en C
=================================================================

âœ“ Dataset cargado exitosamente
  - 60000 imÃ¡genes de 784 caracterÃ­sticas
  - Labels one-hot de 10 clases

MLP creado: 784 -> 512 -> 10 (batch_size=64)

=================================================================
Iniciando entrenamiento (10 epochs, batch_size=64, lr=0.010)
=================================================================

Epoch  1/10 - Loss: 0.4523 - Accuracy: 0.8712 - Time: 12.34s
Epoch  2/10 - Loss: 0.2891 - Accuracy: 0.9124 - Time: 12.21s
...
Epoch 10/10 - Loss: 0.1523 - Accuracy: 0.9512 - Time: 12.18s

=================================================================
Entrenamiento completado en 121.56 segundos
=================================================================

Test Loss: 0.1689 - Test Accuracy: 0.9456

Resultados guardados en: results/raw/c_sequential.csv
```

## ğŸ“ Tareas de ImplementaciÃ³n

### âœ… Ya implementado:

- [x] Estructura del proyecto
- [x] Carga de datos (`data.c`)
- [x] Operaciones bÃ¡sicas de matrices (`matrix.c`)
- [x] InicializaciÃ³n del MLP
- [x] Programa principal de entrenamiento

### ğŸ”§ Por implementar (TÃš):

#### En `mlp.c`:

1. **`mlp_forward()`**:

   - Z1 = X @ W1 + b1
   - A1 = ReLU(Z1)
   - Z2 = A1 @ W2 + b2
   - A2 = Softmax(Z2)

2. **`mlp_backward()`**:

   - dZ2 = A2 - Y
   - dW2 = A1^T @ dZ2
   - db2 = sum(dZ2, axis=0)
   - dA1 = dZ2 @ W2^T
   - dZ1 = dA1 âŠ™ ReLU'(Z1)
   - dW1 = X^T @ dZ1
   - db1 = sum(dZ1, axis=0)

3. **`mlp_update_params()`**:
   - W1 -= lr \* dW1 / batch_size
   - b1 -= lr \* db1 / batch_size
   - W2 -= lr \* dW2 / batch_size
   - b2 -= lr \* db2 / batch_size

## ğŸ› Debug

### Ver valores intermedios

Descomen los `printf` en `mlp.c` o usa `print_matrix()`:

```c
print_matrix("W1", mlp->W1, 5, 5);  // Primeras 5x5 de W1
print_matrix("A2", mlp->A2, batch_size, 10);  // Predicciones
```

### Verificar gradientes

Temporalmente en `mlp_backward()`:

```c
printf("dW1 sum: %.6f\n", sum_all(mlp->dW1, 784 * 512));
printf("dW2 sum: %.6f\n", sum_all(mlp->dW2, 512 * 10));
```

Si los gradientes son NaN o explotan, revisa:

- Softmax tiene overflow â†’ Usa max normalization (ya estÃ¡ implementado)
- Learning rate muy alto â†’ Reduce a 0.001
- Divisiones por cero â†’ Agrega epsilon (1e-10)

## ğŸ“Š Resultados

Los resultados se guardan automÃ¡ticamente en:

```
../../results/raw/c_sequential.csv
```

Formato:

```csv
implementation,language,parallelization,workers_threads,batch_size,epochs,learning_rate,hidden_neurons,total_time_sec,avg_epoch_time,final_loss,final_accuracy,speedup_vs_baseline,notes
c_seq,c,none,1,64,10,0.010,512,121.56,12.16,0.1689,0.9456,1.00,baseline_c
```

## ğŸ” ValidaciÃ³n

### Compara con Python

Tu compaÃ±ero debe ejecutar su versiÃ³n Python secuencial con el mismo seed (42).

Los resultados deben ser similares (diferencia < 1e-3):

```
Python Final Loss:    0.1692  â† OK
C Final Loss:         0.1689  â† OK

Diferencia: 0.0003 âœ“
```

## ğŸš€ PrÃ³ximos Pasos

Una vez que esta versiÃ³n funcione:

1. âœ… Verificar que loss disminuye
2. âœ… Verificar accuracy > 90%
3. âœ… Commit y push
4. ğŸ”„ **Pasar a `c_openmp/`** para paralelizar

## ğŸ’¡ Tips

- Compila con `-O3` para optimizaciÃ³n (ya estÃ¡ en Makefile)
- Usa `valgrind` para detectar memory leaks:
  ```bash
  valgrind --leak-check=full ./bin/train_seq
  ```
- Si es muy lento, reduce epochs o usa subset del dataset

## ğŸ“š Referencias

- FundamentaciÃ³n matemÃ¡tica: `docs/experiment_design.md`
- Formato de datos: `docs/WORKFLOW.md`
