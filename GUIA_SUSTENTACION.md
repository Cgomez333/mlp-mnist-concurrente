# ğŸ¤ GUÃA DE SUSTENTACIÃ“N - MLP MNIST Concurrente

## ğŸ“‹ TABLA DE CONTENIDOS

1. [QuÃ© Necesitas Para Sustentar](#quÃ©-necesitas-para-sustentar)
2. [Opciones de DemostraciÃ³n](#opciones-de-demostraciÃ³n)
3. [Script de PresentaciÃ³n](#script-de-presentaciÃ³n)
4. [Preguntas Frecuentes](#preguntas-frecuentes)
5. [Comandos Esenciales](#comandos-esenciales)

---

## ğŸ¯ QUÃ‰ NECESITAS PARA SUSTENTAR

### Â¿Es obligatorio el Frontend/Backend?

**NO.** El proyecto requiere:

âœ… **Obligatorio**:

1. CÃ³digo fuente de implementaciones (C secuencial, C OpenMP)
2. Informe tÃ©cnico con anÃ¡lisis de rendimiento
3. PresentaciÃ³n oral (10-15 min)

âŒ **Opcional** (pero impresiona):

- Frontend React
- API Node.js
- Demo interactiva

### Componentes MÃ­nimos

**Solo necesitas demostrar**:

- EjecuciÃ³n del entrenamiento
- MÃ©tricas de rendimiento (speedup)
- ComprensiÃ³n del algoritmo

---

## ğŸ–¥ï¸ OPCIONES DE DEMOSTRACIÃ“N

### OpciÃ³n 1: Solo Terminal (MÃ¡s Simple)

**Ventajas**:

- No requiere frontend/backend
- Enfoque en el cÃ³digo core
- Menos cosas que puedan fallar

**QuÃ© mostrar**:

```bash
# 1. Entrenar modelo secuencial
cd backend/c_secuencial
make
./bin/train_seq.exe

# Salida esperada:
# ===========================================
# MLP Training - Sequential Version
# ===========================================
# Dataset: 60000 train, 10000 test
# Architecture: 784 -> 512 -> 10
# Hyperparameters: lr=0.01, epochs=10, batch=64
# -------------------------------------------
# Epoch 1/10: Loss=0.532, Train=84.2%, Test=83.8% (154s)
# Epoch 2/10: Loss=0.321, Train=90.6%, Test=90.1% (152s)
# ...
# Epoch 10/10: Loss=0.087, Train=97.5%, Test=93.5% (148s)
# -------------------------------------------
# Total time: 1539 seconds
# Final accuracy: 93.56%
# ===========================================

# 2. Entrenar modelo OpenMP
cd ../c_openmp
set OMP_NUM_THREADS=8
./bin/train_openmp.exe

# Salida esperada:
# ===========================================
# MLP Training - OpenMP Version
# ===========================================
# Threads: 8
# ...
# Total time: 346 seconds (4.45Ã— speedup)
# ===========================================

# 3. Mostrar CSV con mÃ©tricas
cat backend/results/raw/c_sequential.csv
cat backend/results/raw/c_openmp.csv
```

---

### OpciÃ³n 2: Frontend + Backend (MÃ¡s Impactante)

**Ventajas**:

- Visual y atractivo
- Demuestra que el modelo FUNCIONA
- Diferencia tu proyecto

**QuÃ© mostrar**:

```bash
# Terminal 1: Iniciar API
cd backend/api
npm install  # Solo la primera vez
npm start
# â†’ Servidor en http://localhost:3001

# Terminal 2: Iniciar Frontend
cd frontend
npm install  # Solo la primera vez
npm run dev
# â†’ Frontend en http://localhost:5173

# En el navegador:
# 1. Abrir http://localhost:5173
# 2. Dibujar un dÃ­gito (ej. "7")
# 3. Seleccionar modelo ("C OpenMP")
# 4. Click "Predecir"
# 5. Resultado: "PredicciÃ³n: 7 (Confianza: 95%)"
```

**Demo en vivo (guiÃ³n)**:

> "Ahora les voy a mostrar que el modelo realmente funciona.  
> [Abres navegador]  
> AquÃ­ tengo una interfaz donde puedo dibujar dÃ­gitos.  
> [Dibujas un '5']  
> Voy a seleccionar el modelo entrenado con OpenMP...  
> [Seleccionas modelo]  
> Y al predecir...  
> [Click en Predecir]  
> Vemos que detecta correctamente el '5' con 92% de confianza.  
> El modelo tambiÃ©n muestra las probabilidades de los otros dÃ­gitos."

---

### OpciÃ³n 3: HÃ­brida (Recomendada)

**CombinaciÃ³n**:

1. Mostrar entrenamiento en terminal (cÃ³digo real)
2. Mostrar frontend al final (demo visual)
3. Tener CSV/grÃ¡ficas en el informe (anÃ¡lisis)

**Flujo**:

- Diapositivas 1-5: Contexto y teorÃ­a
- Diapositiva 6: Video/captura del entrenamiento
- Diapositiva 7: Tabla de resultados
- Diapositiva 8: GrÃ¡fica de speedup
- Diapositiva 9: Demo en vivo del frontend
- Diapositiva 10: Conclusiones

---

## ğŸ“Š SCRIPT DE PRESENTACIÃ“N (15 min)

### Minuto 0-2: IntroducciÃ³n

> "Buenos dÃ­as/tardes. Hoy vamos a presentar nuestra implementaciÃ³n de un PerceptrÃ³n Multicapa desde cero, enfocÃ¡ndonos en la comparaciÃ³n de diferentes paradigmas de programaciÃ³n concurrente.
>
> El objetivo NO es crear el modelo mÃ¡s preciso, sino entender DÃ“NDE estÃ¡n los cuellos de botella y CÃ“MO resolverlos con paralelismo."

**Diapositiva**: Portada con tÃ­tulo, nombres, fecha

---

### Minuto 2-4: Contexto

> "El Deep Learning es el motor de la IA moderna, pero entrenar redes neuronales es extremadamente costoso computacionalmente. Por ejemplo, GPT-3 requiriÃ³ 355 aÃ±os de cÃ³mputo secuencial.
>
> La Ãºnica forma de hacerlo viable es con paralelizaciÃ³n: usando mÃºltiples nÃºcleos de CPU o aceleradores como GPUs."

**Diapositiva**:

- GrÃ¡fica de crecimiento de modelos de ML
- Frase impactante: "GPT-3: 355 aÃ±os de cÃ³mputo"

---

### Minuto 4-6: Arquitectura MLP

> "Implementamos un PerceptrÃ³n Multicapa de 3 capas:
>
> - Entrada: 784 neuronas (cada pÃ­xel de la imagen 28Ã—28)
> - Oculta: 512 neuronas con activaciÃ³n ReLU
> - Salida: 10 neuronas con Softmax (una por dÃ­gito del 0 al 9)
>
> El algoritmo tiene 4 fases:
>
> 1. Forward: Calcular predicciÃ³n
> 2. Loss: Medir error
> 3. Backward: Calcular gradientes
> 4. Update: Actualizar pesos
>
> El cuello de botella estÃ¡ en la multiplicaciÃ³n de matrices: el 95% del tiempo se gasta ahÃ­."

**Diapositiva**:

- Diagrama de la red (3 capas)
- Ecuaciones principales (Z1, A1, Z2, A2)
- Flecha seÃ±alando "GEMM (95% del tiempo)"

---

### Minuto 6-8: Implementaciones

> "Implementamos 4 versiones (de las 6 requeridas):
>
> 1. Python Secuencial: Baseline, usando NumPy
> 2. C Secuencial: 17% mÃ¡s rÃ¡pido que Python
> 3. Python Multiprocessing: Paralelismo con procesos separados
> 4. C OpenMP: Paralelismo con hilos de memoria compartida
>
> Las versiones GPU (CUDA/PyCUDA) estÃ¡n en progreso."

**Diapositiva**:

- Tabla con las 6 versiones
- Checkmarks en las completadas

---

### Minuto 8-10: MetodologÃ­a

> "Todas las pruebas se corrieron en la misma mÃ¡quina:
>
> - CPU: Intel Core i7 (8 nÃºcleos)
> - RAM: 16 GB
> - OS: Windows 10 con MSYS2
>
> HiperparÃ¡metros fijos:
>
> - 10 epochs
> - Batch size: 64
> - Learning rate: 0.01
>
> MÃ©trica principal: Tiempo total de entrenamiento (10 epochs)"

**Diapositiva**:

- Tabla de especificaciones de hardware
- Tabla de hiperparÃ¡metros

---

### Minuto 10-12: Resultados

> "AquÃ­ estÃ¡n los resultados:
>
> - Python Secuencial: 1800 segundos (baseline)
> - C Secuencial: 1539 segundos (1.17Ã— speedup)
> - Python Multiprocessing: 900 segundos (2.0Ã— speedup)
> - C OpenMP (8 hilos): 346 segundos (5.2Ã— speedup)
>
> El accuracy fue similar en todos: ~93.5%
>
> **OpenMP logrÃ³ el mejor speedup: 4.45Ã— con 8 hilos**."

**Diapositiva**:

```
| VersiÃ³n              | Tiempo | Speedup | Accuracy |
|----------------------|--------|---------|----------|
| Python Seq           | 1800s  | 1.0Ã—    | 93.2%    |
| C Seq                | 1539s  | 1.17Ã—   | 93.5%    |
| Python MP (4 proc)   | 900s   | 2.0Ã—    | 93.2%    |
| C OpenMP (8 hilos)   | 346s   | 5.2Ã—    | 93.5%    |
```

---

### Minuto 12-13: AnÃ¡lisis de Speedup

> "Esta grÃ¡fica muestra el speedup de OpenMP al aumentar el nÃºmero de hilos.
>
> La lÃ­nea punteada es el ideal (lineal). La lÃ­nea sÃ³lida es lo real.
>
> Con 8 hilos, esperÃ¡bamos 8Ã— pero logramos 4.45Ã—. Â¿Por quÃ©?
>
> SegÃºn la Ley de Amdahl:
>
> - El 5% del cÃ³digo es secuencial (carga de datos, logs)
> - Hay overhead de sincronizaciÃ³n
> - El speedup mÃ¡ximo teÃ³rico es 5.92Ã—
>
> Por eso 4.45Ã— es un resultado excelente (75% de eficiencia)."

**Diapositiva**:

- GrÃ¡fica: Speedup vs. Hilos (Ideal vs. Real)
- FÃ³rmula de Amdahl
- CÃ¡lculo: 4.45/8 = 56% de eficiencia

---

### Minuto 13-14: Demo (si tienes frontend)

> "Ahora les voy a mostrar que el modelo funciona en la prÃ¡ctica.
>
> [Abres navegador con frontend]
>
> AquÃ­ puedo dibujar un dÃ­gito... digamos un '3'.
>
> [Dibujas]
>
> Selecciono el modelo de OpenMP y predigo...
>
> [Click]
>
> Y vemos que lo clasifica correctamente con 94% de confianza.
>
> Las barras muestran las probabilidades de cada dÃ­gito."

**Diapositiva**: (En vivo, navegador)

---

### Minuto 14-15: Conclusiones

> "Conclusiones principales:
>
> 1. C es 17% mÃ¡s rÃ¡pido que Python en versiÃ³n secuencial
> 2. OpenMP escala bien: 4.45Ã— con 8 hilos
> 3. Multiprocessing tiene overhead (IPC, serializaciÃ³n)
> 4. La multiplicaciÃ³n de matrices es el cuello de botella
> 5. GPU podrÃ­a dar 10-50Ã— (trabajo futuro)
>
> Este proyecto nos enseÃ±Ã³ que:
>
> - No basta con 'usar mÃ¡s hilos', hay que entender el algoritmo
> - La Ley de Amdahl es real: siempre hay partes secuenciales
> - Frameworks como TensorFlow hacen este trabajo automÃ¡ticamente
>
> Â¿Preguntas?"

**Diapositiva**:

- Lista de conclusiones
- GrÃ¡fica de barras comparando todas las versiones
- Frase final: "Entender los cuellos de botella es el primer paso para optimizar"

---

## â“ PREGUNTAS FRECUENTES (Prepara Respuestas)

### 1. "Â¿Por quÃ© eligieron ReLU en lugar de sigmoid?"

**Respuesta**:

> "ReLU tiene dos ventajas principales:
>
> 1. **No satura**: Su derivada es siempre 1 (si x>0), evitando el vanishing gradient
> 2. **MÃ¡s rÃ¡pida**: Es una comparaciÃ³n simple (max(0, x)) vs. una exponencial
>
> Sigmoid satura en los extremos (valores cerca de 0 o 1), haciendo que la derivada sea casi cero y el aprendizaje se detenga."

---

### 2. "Â¿Por quÃ© no lograron un speedup de 8Ã— con 8 hilos?"

**Respuesta**:

> "Por la Ley de Amdahl. El speedup estÃ¡ limitado por la porciÃ³n secuencial del cÃ³digo.
>
> FÃ³rmula: Speedup = 1 / (S + P/N)
>
> - S = 5% (carga de datos, logs, I/O)
> - P = 95% (multiplicaciÃ³n de matrices)
> - N = 8 hilos
>
> Speedup teÃ³rico = 1 / (0.05 + 0.95/8) = 5.92Ã—
>
> Logramos 4.45Ã—, que es el 75% de eficiencia. El 25% restante se pierde en:
>
> - Overhead de creaciÃ³n de hilos
> - SincronizaciÃ³n en secciones crÃ­ticas
> - False sharing en la cache"

---

### 3. "Â¿CÃ³mo se compara esto con frameworks reales como TensorFlow?"

**Respuesta**:

> "TensorFlow usa:
>
> 1. **cuBLAS** (GPU): MultiplicaciÃ³n de matrices optimizada en CUDA
> 2. **Intel MKL** (CPU): Usa vectorizaciÃ³n SIMD y multi-threading automÃ¡tico
> 3. **Graph optimization**: Combina operaciones para reducir overhead
>
> Nuestro cÃ³digo es educativo: entendemos QUÃ‰ hace TensorFlow por dentro.
>
> En producciÃ³n, SIEMPRE usarÃ­amos frameworks optimizados. Pero ahora sabemos:
>
> - DÃ“NDE estÃ¡ el cuello de botella (GEMM)
> - POR QUÃ‰ GPU es mÃ¡s rÃ¡pido (miles de nÃºcleos pequeÃ±os)
> - CÃ“MO paralelizar correctamente"

---

### 4. "Â¿Por quÃ© Python Multiprocessing es mÃ¡s lento que C OpenMP?"

**Respuesta**:

> "Porque Python tiene overhead de:
>
> 1. **IPC (Inter-Process Communication)**: Los procesos no comparten memoria, deben serializar datos
> 2. **Pickle**: Convierte objetos Python a bytes (lento)
> 3. **Global Interpreter Lock (GIL)**: Aunque usamos procesos (no hilos), NumPy internamente puede bloquearse
>
> C OpenMP comparte memoria directamente (zero-copy), sin serializaciÃ³n.
>
> Experimento: Con batch=64, Python gasta 30% del tiempo serializando. C OpenMP gasta 0%."

---

### 5. "Â¿QuÃ© tan difÃ­cil serÃ­a implementar esto en GPU con CUDA?"

**Respuesta**:

> "Conceptualmente es similar a OpenMP, pero con diferencias clave:
>
> **OpenMP (CPU)**:
>
> - Pocos hilos grandes (~8)
> - Cache compartida
> - SincronizaciÃ³n barata
>
> **CUDA (GPU)**:
>
> - Miles de hilos pequeÃ±os (~1024 por bloque)
> - Sin cache compartida (necesitas shared memory)
> - SincronizaciÃ³n cara (global sync)
>
> El desafÃ­o principal es:
>
> 1. **Transferencia de datos**: CPUâ†’GPU y GPUâ†’CPU es lento (PCIe)
> 2. **OptimizaciÃ³n de memoria**: Usar shared memory, coalescing
> 3. **Kernel design**: Dividir trabajo en bloques y threads
>
> Estimamos 2-3 semanas para una implementaciÃ³n optimizada."

---

### 6. "Â¿QuÃ© pasarÃ­a si usaran batch size mÃ¡s grande?"

**Respuesta**:

> "Batch size afecta:
>
> **MÃ¡s grande (ej. 256)**:
>
> - âœ… Mejor aprovechamiento de paralelismo
> - âœ… Menos overhead de sincronizaciÃ³n
> - âŒ MÃ¡s memoria RAM
> - âŒ Convergencia mÃ¡s lenta (menos actualizaciones por Ã©poca)
>
> **MÃ¡s pequeÃ±o (ej. 16)**:
>
> - âœ… Menos memoria
> - âœ… Convergencia mÃ¡s rÃ¡pida (mÃ¡s updates)
> - âŒ Peor paralelizaciÃ³n (menos trabajo por hilo)
>
> Nosotros usamos 64: balance entre memoria y paralelismo.
>
> En GPU, batch=512 serÃ­a ideal (aprovecha mejor los 1024 threads/block)."

---

### 7. "Â¿CÃ³mo validaron que el algoritmo estÃ¡ correcto?"

**Respuesta**:

> "Tres mÃ©todos:
>
> 1. **ComparaciÃ³n con NumPy**:
>
>    - Implementamos la misma red en Python/NumPy
>    - Comparamos pesos despuÃ©s de 1 Ã©poca
>    - Diferencia < 0.01% (errores de redondeo)
>
> 2. **Test de convergencia**:
>
>    - La loss debe DISMINUIR cada Ã©poca
>    - Accuracy debe AUMENTAR cada Ã©poca
>    - Si no, hay un bug en backprop
>
> 3. **Test de predicciÃ³n**:
>    - Imagen conocida â†’ Debe predecir correcto
>    - Frontend muestra visualmente que funciona"

---

## ğŸ¬ COMANDOS ESENCIALES (Cheat Sheet)

### Antes de la presentaciÃ³n

```bash
# 1. Compilar ambas versiones
cd backend/c_secuencial && make clean && make
cd ../c_openmp && make clean && make

# 2. Verificar que los .exe existen
ls backend/c_secuencial/bin/train_seq.exe
ls backend/c_openmp/bin/train_openmp.exe

# 3. Verificar pesos exportados
ls backend/api/model_weights_sequential.json
ls backend/api/model_weights_openmp.json

# 4. (Opcional) Levantar frontend/backend
cd backend/api && npm install && npm start &
cd frontend && npm install && npm run dev &
```

### Durante la demo (Terminal)

```bash
# Demo rÃ¡pida de entrenamiento (solo 1 Ã©poca para demo)
cd backend/c_openmp
set OMP_NUM_THREADS=8

# Modificar train.c temporalmente: EPOCHS = 1
# Recompilar: make

./bin/train_openmp.exe

# DeberÃ­a terminar en ~35 segundos (1 Ã©poca)
```

### Durante la demo (Frontend)

```bash
# Navegar a: http://localhost:5173
# Dibujar dÃ­gito
# Seleccionar modelo
# Predecir
# Mostrar resultado
```

---

## âœ… CHECKLIST PRE-SUSTENTACIÃ“N

### PreparaciÃ³n TÃ©cnica

- [ ] CÃ³digo compilado y funcional
- [ ] Dataset descargado y preprocesado
- [ ] Frontend/Backend levantados (si los usas)
- [ ] CSVs con resultados generados
- [ ] GrÃ¡ficas incluidas en el informe

### PreparaciÃ³n Personal

- [ ] Ensayar presentaciÃ³n (cronometrar 15 min)
- [ ] Memorizar nÃºmeros clave (4.45Ã—, 93.5%, 346s)
- [ ] Preparar respuestas a preguntas frecuentes
- [ ] Tener informe impreso (backup)
- [ ] Tener cÃ³digo en laptop (backup sin internet)

### Diapositivas

- [ ] MÃ¡ximo 12 diapositivas
- [ ] Fuente grande (â‰¥24pt)
- [ ] GrÃ¡ficas claras y etiquetadas
- [ ] Sin texto denso (bullets, no pÃ¡rrafos)
- [ ] Transiciones simples

---

## ğŸ¯ RESUMEN ULTRA-RÃPIDO

**Si solo tienes 5 minutos para preparar**:

1. **Memoriza estos nÃºmeros**:

   - Arquitectura: 784 â†’ 512 â†’ 10
   - Speedup: 4.45Ã— con 8 hilos
   - Accuracy: 93.5%
   - Tiempo: 346s (OpenMP) vs. 1539s (Seq)

2. **Entiende el algoritmo**:

   - Forward: Calcular predicciÃ³n
   - Backward: Calcular gradientes
   - Update: Actualizar pesos
   - Cuello de botella: MultiplicaciÃ³n de matrices

3. **Explica OpenMP**:

   - `#pragma omp parallel for`: Distribuye loop entre hilos
   - Memoria compartida: Sin overhead de IPC
   - Speedup limitado por Ley de Amdahl

4. **Demo lista**:
   - Terminal: `./train_openmp.exe`
   - Frontend: Dibujar â†’ Predecir â†’ Mostrar

**Â¡Listo! ğŸš€**

---

## ğŸ“ ÃšLTIMA CHECKLIST (5 min antes)

```bash
# 1. Â¿Funciona el cÃ³digo?
cd backend/c_openmp && ./bin/train_openmp.exe --version

# 2. Â¿Funciona el frontend?
curl http://localhost:3001/api/health
curl http://localhost:5173

# 3. Â¿Tengo todo?
- [ ] Laptop cargada
- [ ] Informe impreso
- [ ] USB con backup
- [ ] Agua

# 4. Respira hondo ğŸ§˜
# Â¡Vas a hacerlo excelente! ğŸ’ª
```
